{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fuel_econ = pd.read_csv('fuel_econ.csv')\n",
        "\n",
        "# Handle non-numeric columns in fuel_econ dataset\n",
        "non_numeric_cols = fuel_econ.select_dtypes(exclude=[np.number]).columns\n",
        "print(\"Non-numeric columns in fuel_econ:\", non_numeric_cols)\n",
        "\n",
        "# Option 1: Exclude non-numeric columns\n",
        "fuel_econ_numeric = fuel_econ.select_dtypes(include=[np.number])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RNeKvXE13nq",
        "outputId": "565ac04d-f83d-4ac5-a909-872f4b3242fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-numeric columns in fuel_econ: Index(['make', 'model', 'VClass', 'drive', 'trans', 'fuelType'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "Standarizing data is important because it helps ensure that all variables are treated equaly and that no single variable dominates the the analysis"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "source": [
        "# Calculate covariance matrix\n",
        "cov_matrix = np.cov(standardized_data.T)\n",
        "\n",
        "print() # Change 'Print()' to 'print()' to call the built-in print function"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NvFT45L4clI",
        "outputId": "3bd5fded-1a9e-4758-c05f-439825b47ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "# Use order_of_importance instead of sorted_indices\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "# Use order_of_importance instead of sorted_indices\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Y_BVKr4KUG",
        "outputId": "e4899126-14ce-4382-fa42-c6e35d739e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "Ordering eigenvalues and their corresponding eigenvectors is important in PCA because the magnitude of each eigenvalue represents the amount of variance captured by its associated eigenvector (principal component). By arranging them in descending order, we can prioritize the principal components that capture the most variance. This allows us to reduce the dimensionality of the data while retaining the most significant information, as we focus on the components that explain the majority of the variance.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "No, in PCA, we generally do not prioritize the lowest eigenvalues because they represent components that capture the least variance in the data. The goal of PCA is to reduce the dimensionality while preserving as much variance as possible, which is why we focus on the highest eigenvalues. By selecting the components with higher eigenvalues, we retain the directions in the data that explain the most variance, thus preserving essential patterns and structure. Lower eigenvalues correspond to directions with minimal variance and are often disregarded to reduce noise and simplify the model."
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate explained variance percentages for each eigenvalue\n",
        "explained_variance = (sorted_eigenvalues / np.sum(sorted_eigenvalues)) * 100\n",
        "\n",
        "# Format the explained variance percentages to two decimal places\n",
        "explained_variance = [\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "\n",
        "# Print the explained variance percentages\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "177a3d77-13c0-4059-882f-e89c84f465fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'data' is your original dataset\n",
        "# Replace 'data' with the actual variable name of your dataset\n",
        "# Load your dataset or create a sample dataset\n",
        "# Example: Load data from a CSV file\n",
        "# data = np.genfromtxt('your_data.csv', delimiter=',')\n",
        "# Example: Create a sample dataset\n",
        "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "\n",
        "# Calculate the mean and standard deviation of each column (feature)\n",
        "data_mean = np.mean(data, axis=0)\n",
        "data_std = np.std(data, axis=0)\n",
        "\n",
        "# Standardize the data by subtracting the mean and dividing by the standard deviation\n",
        "standardized_data = (data - data_mean) / data_std\n",
        "\n",
        "k = 2\n",
        "\n",
        "# You need to define and calculate 'sorted_eigenvectors' before using it\n",
        "# For example, you might have obtained it from PCA:\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA()\n",
        "# pca.fit(standardized_data)\n",
        "# sorted_eigenvectors = pca.components_\n",
        "\n",
        "# Assuming you have 'sorted_eigenvectors' calculated, uncomment the following line:\n",
        "# reduced_data = np.matmul(standardized_data, sorted_eigenvectors[:, :k])\n",
        "\n",
        "# Placeholder for demonstration purposes - Replace with your actual sorted_eigenvectors\n",
        "sorted_eigenvectors = np.eye(3)  # Example: Using an identity matrix\n",
        "\n",
        "# Now you can calculate reduced_data\n",
        "reduced_data = np.matmul(standardized_data, sorted_eigenvectors[:, :k])\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "x5EW221nWRF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "16743692-89f9-4456-ef6b-4c6c7398d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "940ca8d2-d179-4a41-fd77-49586743c44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give two benefits and 2 limitations\n",
        "\n",
        "**Benefits of PCA:**\n",
        "1. **Dimensionality Reduction**: PCA reduces the number of features while retaining most of the dataset's information, improving computation speed and simplifying visualization.\n",
        "2. **Noise Reduction**: By focusing on principal components that capture the most variance, PCA can help filter out noise and irrelevant information.\n",
        "\n",
        "**Limitations of PCA:**\n",
        "1. **Information Loss**: Reducing dimensions may discard some information, especially if we retain fewer components.\n",
        "2. **Linear Relationships**: PCA assumes linear relationships between features, so it may not be effective for data with complex, non-linear structures.\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}